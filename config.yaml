# Configuration for browser agent

# LLM Settings
llm:
  provider: "openai"  # openai, anthropic, local
  model: "gpt-4o-mini"
  temperature: 0.7
  max_tokens: 4096
  
  # For local LLM (e.g., llama.cpp, vLLM, Ollama with OpenAI compatibility)
  local:
    base_url: "http://localhost:8000/v1"
    # Recommended lightweight models for CPU:
    # - phi-3-mini (3.8B)
    # - mistral-7b-instruct
    # - llama-3.2-3b-instruct

# Browser Settings
browser:
  headless: false
  screen_size:
    width: 1440
    height: 900
  initial_url: "https://www.google.com"
  
  # User agent (optional)
  user_agent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"

# DOM Extraction Settings
dom:
  max_text_length: 200  # Maximum text length per element
  max_elements: 100     # Maximum number of elements to extract
  
  # Filter settings
  exclude_invisible: true
  only_interactive: true

# Agent Settings
agent:
  max_iterations: 20
  verbose: true
  
  # Safety settings
  confirm_navigation: false  # Ask before navigating to external sites
  allowed_domains: []        # Empty = allow all

# Vision Settings
vision:
  enabled: true
  use_yolo: false
  model: "yolov8n"  # Nano model for CPU (requires ultralytics, set use_yolo: true)
  confidence_threshold: 0.5

# Model Orchestrator - dynamically selects model based on context
orchestrator:
  enabled: true
  dom_sparse_threshold: 5       # element count below which vision activates
  stuck_iteration_threshold: 3  # same URL for N iters â†’ escalate to vision

  # LLM Router: tiny model (1-2B) that classifies which tier to use each iteration
  # Set enabled: true to use LLM-based routing instead of rule-based selection
  router:
    enabled: false              # set true to activate LLM router
    model: "gemma3:1b"
    base_url: "http://localhost:11434/v1"
    timeout_sec: 8.0

  tiers:
    ultra_fast:
      model: "lfm2.5-thinking:latest"
      base_url: "http://localhost:11434/v1"
      supports_vision: false
      max_tokens: 1024
      temperature: 0.3
    balanced:
      model: "ministral-3:3b"
      base_url: "http://localhost:11434/v1"
      supports_vision: false
      max_tokens: 2048
      temperature: 0.5
    quick_vision:
      model: "granite3.2-vision:latest"
      base_url: "http://localhost:11434/v1"
      supports_vision: true
      max_tokens: 2048
      temperature: 0.5
    full_vision:
      model: "qwen3-vl:4b"
      base_url: "http://localhost:11434/v1"
      supports_vision: true
      max_tokens: 4096
      temperature: 0.7

# Available models for benchmark (not used by orchestrator directly)
benchmark_models:
  - name: "ministral-3:3b"
    size_gb: 3.0
    type: "text"
  - name: "granite4:3b"
    size_gb: 2.1
    type: "text"
  - name: "gemma3:4b"
    size_gb: 3.3
    type: "text"
  - name: "ministral-3:8b"
    size_gb: 6.0
    type: "text"
  - name: "qwen3-vl:4b"
    size_gb: 3.3
    type: "vision"
  - name: "qwen3-vl:8b"
    size_gb: 6.1
    type: "vision"
  - name: "granite3.2-vision:latest"
    size_gb: 2.4
    type: "vision"
  
# Logging
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  save_conversations: true
  conversation_dir: "./conversations"
  save_screenshots: false
  screenshot_dir: "./screenshots"
